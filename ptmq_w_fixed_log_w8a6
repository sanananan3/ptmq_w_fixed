/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/content/ptmq_w_fixed/utils/model_utils.py:163: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(config.path, map_location='cpu')
 <class 'models.resnet.ResNet'>
conv1 <class 'quant.quant_module.QuantizedLayer'>
conv1.module <class 'quant.quant_module.QConv2d'>
conv1.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
conv1.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
conv1.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
conv1.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
conv1.activation <class 'torch.nn.modules.activation.ReLU'>
bn1 <class 'utils.fold_bn.StraightThrough'>
relu <class 'utils.fold_bn.StraightThrough'>
maxpool <class 'torch.nn.modules.pooling.MaxPool2d'>
layer1 <class 'torch.nn.modules.container.Sequential'>
layer1.0 <class 'utils.model_utils.QuantBasicBlock'>
layer1.0.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer1.0.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer1.0.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.0.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.0.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer1.0.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer1.0.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.0.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer1.1 <class 'utils.model_utils.QuantBasicBlock'>
layer1.1.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer1.1.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer1.1.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.1.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.1.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.1.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer1.1.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer1.1.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.1.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer2 <class 'torch.nn.modules.container.Sequential'>
layer2.0 <class 'utils.model_utils.QuantBasicBlock'>
layer2.0.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer2.0.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer2.0.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.0.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer2.0.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer2.0.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer2.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer2.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer2.1 <class 'utils.model_utils.QuantBasicBlock'>
layer2.1.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer2.1.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer2.1.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.1.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.1.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.1.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer2.1.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer2.1.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.1.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer3 <class 'torch.nn.modules.container.Sequential'>
layer3.0 <class 'utils.model_utils.QuantBasicBlock'>
layer3.0.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer3.0.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer3.0.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.0.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer3.0.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer3.0.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer3.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer3.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer3.1 <class 'utils.model_utils.QuantBasicBlock'>
layer3.1.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer3.1.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer3.1.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.1.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.1.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.1.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer3.1.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer3.1.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.1.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer4 <class 'torch.nn.modules.container.Sequential'>
layer4.0 <class 'utils.model_utils.QuantBasicBlock'>
layer4.0.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer4.0.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer4.0.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.0.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer4.0.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer4.0.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer4.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer4.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer4.1 <class 'utils.model_utils.QuantBasicBlock'>
layer4.1.conv1_relu_low <class 'quant.quant_module.QuantizedLayer'>
layer4.1.conv1_relu_low.module <class 'quant.quant_module.QConv2d'>
layer4.1.conv1_relu_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.1.conv1_relu_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.1.conv1_relu_low.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.1.conv2_low <class 'quant.quant_module.QuantizedLayer'>
layer4.1.conv2_low.module <class 'quant.quant_module.QConv2d'>
layer4.1.conv2_low.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.1.conv2_low.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
avgpool <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>
fc <class 'quant.quant_module.QuantizedLayer'>
fc.module <class 'quant.quant_module.QLinear'>
fc.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
fc.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0 calib
layer1.1 calib
layer2.0 calib
layer2.1 calib
layer3.0 calib
layer3.1 calib
layer4.0 calib
layer4.1 calib
[AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
), AdaRoundFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
  (observer): MSEObserver()
)]
[LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
), LSQFakeQuantize(
  fake_quant_enabled=0, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
  (observer): MSEObserver()
)]
Starting model calibration...
/content/ptmq_w_fixed/quant/observer.py:183: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:71.)
  x_min, x_max = torch._aminmax(x)
/content/ptmq_w_fixed/quant/observer.py:153: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:687.)
  x_min, x_max = torch._aminmax(y, 1)
Completed model calibration
Starting block reconstruction...
QuantizedLayer(
  (module): QConv2d(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)
    (weight_fake_quant): AdaRoundFakeQuantize(
      fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
      (observer): MSEObserver()
    )
  )
  (layer_post_act_fake_quantize): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
  (activation): ReLU(inplace=True)
)
Reconstruction with GD Loss: False...: 100% 5000/5000 [01:22<00:00, 60.73it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [02:21<00:00, 35.30it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [02:20<00:00, 35.49it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (downsample): QuantizedLayer(
    (module): QConv2d(
      64, 128, kernel_size=(1, 1), stride=(2, 2)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [01:45<00:00, 47.46it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [01:28<00:00, 56.54it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (downsample): QuantizedLayer(
    (module): QConv2d(
      128, 256, kernel_size=(1, 1), stride=(2, 2)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [01:33<00:00, 53.25it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [01:20<00:00, 62.41it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (downsample): QuantizedLayer(
    (module): QConv2d(
      256, 512, kernel_size=(1, 1), stride=(2, 2)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=6, ch_axis=-1, quant_min=0, quant_max=63
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [01:51<00:00, 44.95it/s]
QuantBasicBlock(
  (conv1_relu_low): QuantizedLayer(
    (module): QConv2d(
      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
    (activation): ReLU(inplace=True)
  )
  (conv2_low): QuantizedLayer(
    (module): QConv2d(
      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (weight_fake_quant): AdaRoundFakeQuantize(
        fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
        (observer): MSEObserver()
      )
    )
  )
  (activation): ReLU(inplace=True)
  (block_post_act_fake_quantize_med): LSQFakeQuantize(
    fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=-1, quant_min=0, quant_max=255
    (observer): MSEObserver()
  )
)
Reconstruction with GD Loss: True...: 100% 5000/5000 [01:45<00:00, 47.44it/s]
QuantizedLayer(
  (module): QLinear(
    in_features=512, out_features=1000, bias=True
    (weight_fake_quant): AdaRoundFakeQuantize(
      fake_quant_enabled=1, observer_enabled=0, symmetric=False, bit=8, ch_axis=0, quant_min=0, quant_max=255
      (observer): MSEObserver()
    )
  )
)
Reconstruction with GD Loss: False...: 100% 5000/5000 [00:13<00:00, 365.67it/s]
Completed block reconstruction
PTMQ block reconstruction took 980.76 seconds
imagenet_val: 100% 196/196 [06:24<00:00,  1.96s/it]
 * Acc@1 69.662 Acc@5 88.954